{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f014ade8",
   "metadata": {},
   "source": [
    "# **Plant Disease Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105ab64",
   "metadata": {},
   "source": [
    "## Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ff1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3312a5a",
   "metadata": {},
   "source": [
    "\n",
    "* tensorflow is imported as tf.\n",
    "* The models and layers sub-modules are imported from tensorflow.keras. These sub-modules provide functions for creating and training neural network models, as well as building and configuring different types of layers.\n",
    "* matplotlib.pyplot is imported as plt, which allows you to create different types of plots and visualizations to analyze your data.\n",
    "* IPython.display.HTML is imported to display HTML content in the Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc3329",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f071a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c5dff",
   "metadata": {},
   "source": [
    "## Data Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481c5019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60060 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aff553",
   "metadata": {},
   "source": [
    "* This code uses the ImageDataGenerator class from tensorflow.keras.preprocessing.image module to perform data augmentation on image data during training of a deep learning model. Data augmentation helps to artificially increase the size of the dataset by creating new, slightly modified versions of the existing images, which helps to improve the robustness of the model and reduce overfitting.\n",
    "\n",
    "* The train_datagen object is created with several parameters to specify the types of image augmentations to apply, such as rotating the image by a random angle between -10 and 10 degrees, and randomly flipping the image horizontally. The rescale parameter is used to normalize the pixel values in the image to be between 0 and 1.\n",
    "\n",
    "* The train_generator object is then created using the flow_from_directory() method, which takes the path to the training directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory, apply the specified augmentations, and generate batches of augmented images for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8172c",
   "metadata": {},
   "source": [
    "## Class Names with Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8011f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple___Black_rot': 0,\n",
       " 'Apple___Cedar_apple_rust': 1,\n",
       " 'Apple___healthy': 2,\n",
       " 'Corn_(maize)___Common_rust_': 3,\n",
       " 'Corn_(maize)___Northern_Leaf_Blight': 4,\n",
       " 'Corn_(maize)___healthy': 5,\n",
       " 'Grape___Black_rot': 6,\n",
       " 'Grape___Esca_(Black_Measles)': 7,\n",
       " 'Grape___healthy': 8,\n",
       " 'Potato___Early_blight': 9,\n",
       " 'Potato___Late_blight': 10,\n",
       " 'Potato___healthy': 11,\n",
       " 'Tomato___Bacterial_spot': 12,\n",
       " 'Tomato___Early_blight': 13,\n",
       " 'Tomato___Late_blight': 14,\n",
       " 'Tomato___healthy': 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2cd85",
   "metadata": {},
   "source": [
    "## Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b61f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Corn_(maize)___Common_rust_',\n",
       " 'Corn_(maize)___Northern_Leaf_Blight',\n",
       " 'Corn_(maize)___healthy',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___Late_blight',\n",
       " 'Potato___healthy',\n",
       " 'Tomato___Bacterial_spot',\n",
       " 'Tomato___Early_blight',\n",
       " 'Tomato___Late_blight',\n",
       " 'Tomato___healthy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98f22f",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e159071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15016 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'dataset/val',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a7c9d",
   "metadata": {},
   "source": [
    "* This code creates a validation generator similar to the train_generator, but for the validation set. The validation_datagen object is created with the same set of parameters as train_datagen for consistency.\n",
    "\n",
    "* The validation_generator is created using the flow_from_directory() method, which takes the path to the validation directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for validation during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eae32c",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df606de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa68d0",
   "metadata": {},
   "source": [
    "* This code creates a test generator similar to the train_generator and validation_generator. The test_datagen object is created with the same set of parameters as train_datagen and validation_datagen for consistency.\n",
    "\n",
    "* The test_generator is created using the flow_from_directory() method, which takes the path to the test directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for testing the model after training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae35dd7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a7c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 16\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aa21f0d",
   "metadata": {},
   "source": [
    "* The model is created using the Sequential API from Keras, which allows us to stack layers sequentially to define the architecture of the CNN model.\n",
    "* The InputLayer specifies the shape of the input tensor to the model, which is determined by the input_shape variable.\n",
    "* The Conv2D layers are convolutional layers that apply filters (also known as kernels) to the input images to extract local patterns or features. The first Conv2D layer has 32 filters, the second has 64 filters, and the third has 128 filters. All three Conv2D layers use a 3x3 kernel and the relu activation function, which introduces non-linearity to the model.\n",
    "* The MaxPooling2D layers perform max pooling operation, which downsamples the feature maps by selecting the maximum value from a 2x2 window. This helps to reduce the spatial dimensions and capture the most important features.\n",
    "* The Flatten layer is used to convert the feature maps into a 1D array. This flattening operation is necessary to feed the data into a fully connected layer.\n",
    "* The Dense layer is a fully connected layer that has 32 units and uses the relu activation function. It takes the flattened feature maps as input and applies weights and biases to produce output values.\n",
    "* The Dropout layer is used to mitigate overfitting, which is a common issue in deep neural networks. It randomly sets a certain percentage (in this case, 50%) of the input units to 0 at each training iteration to prevent the model from relying too much on any single input unit.\n",
    "* The final Dense layer has n_classes units, where n_classes is the number of classes in the dataset. It uses the softmax activation function, which produces a probability distribution over the classes as the output of the model. The class with the highest probability is predicted as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b7d44",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b3d8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 115200)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                3686432   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,780,208\n",
      "Trainable params: 3,780,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9b80e",
   "metadata": {},
   "source": [
    "## Model Compile with Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160e9a0",
   "metadata": {},
   "source": [
    "### This code compiles the CNN model defined in the previous code block using the compile() method.\n",
    "\n",
    "* The optimizer parameter is set to 'adam', which is an optimization algorithm that is commonly used for deep learning models.\n",
    "\n",
    "* The loss parameter is set to tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), which is the loss function used to measure the difference between the predicted and actual labels. SparseCategoricalCrossentropy is used because the labels are in integer form, and not one-hot encoded.\n",
    "\n",
    "* The metrics parameter is set to 'accuracy', which specifies that the accuracy of the model will be used as the evaluation metric during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dceef",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76ae58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1800/1800 [==============================] - 656s 359ms/step - loss: 1.5737 - accuracy: 0.4480 - val_loss: 0.6728 - val_accuracy: 0.7917\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 637s 354ms/step - loss: 1.0072 - accuracy: 0.6365 - val_loss: 0.3805 - val_accuracy: 0.8912\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.8193 - accuracy: 0.7065 - val_loss: 0.3184 - val_accuracy: 0.9032\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.7129 - accuracy: 0.7445 - val_loss: 0.2287 - val_accuracy: 0.9287\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 635s 353ms/step - loss: 0.6433 - accuracy: 0.7679 - val_loss: 0.1844 - val_accuracy: 0.9412\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.5986 - accuracy: 0.7865 - val_loss: 0.2105 - val_accuracy: 0.9318\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.5430 - accuracy: 0.8103 - val_loss: 0.1678 - val_accuracy: 0.9461\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 627s 349ms/step - loss: 0.5054 - accuracy: 0.8206 - val_loss: 0.1815 - val_accuracy: 0.9471\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.4500 - accuracy: 0.8439 - val_loss: 0.1228 - val_accuracy: 0.9602\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.4281 - accuracy: 0.8500 - val_loss: 0.1137 - val_accuracy: 0.9662\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.4153 - accuracy: 0.8586 - val_loss: 0.1368 - val_accuracy: 0.9569\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.3948 - accuracy: 0.8638 - val_loss: 0.1102 - val_accuracy: 0.9691\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.3681 - accuracy: 0.8727 - val_loss: 0.0912 - val_accuracy: 0.9719\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.3711 - accuracy: 0.8714 - val_loss: 0.1073 - val_accuracy: 0.9657\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 627s 348ms/step - loss: 0.3568 - accuracy: 0.8759 - val_loss: 0.0955 - val_accuracy: 0.9728\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.3426 - accuracy: 0.8827 - val_loss: 0.1083 - val_accuracy: 0.9691\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.3291 - accuracy: 0.8861 - val_loss: 0.0929 - val_accuracy: 0.9736\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.3198 - accuracy: 0.8920 - val_loss: 0.0879 - val_accuracy: 0.9742\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.2840 - accuracy: 0.9054 - val_loss: 0.0773 - val_accuracy: 0.9765\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.2677 - accuracy: 0.9101 - val_loss: 0.0812 - val_accuracy: 0.9758\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.2772 - accuracy: 0.9074 - val_loss: 0.0765 - val_accuracy: 0.9787\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.2587 - accuracy: 0.9128 - val_loss: 0.0793 - val_accuracy: 0.9787\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.2550 - accuracy: 0.9145 - val_loss: 0.0768 - val_accuracy: 0.9752\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.2542 - accuracy: 0.9166 - val_loss: 0.1931 - val_accuracy: 0.9419\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 627s 348ms/step - loss: 0.2452 - accuracy: 0.9181 - val_loss: 0.0914 - val_accuracy: 0.9754\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.2285 - accuracy: 0.9226 - val_loss: 0.0748 - val_accuracy: 0.9806\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.2559 - accuracy: 0.9151 - val_loss: 0.0762 - val_accuracy: 0.9785\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.2276 - accuracy: 0.9250 - val_loss: 0.0678 - val_accuracy: 0.9801\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.2216 - accuracy: 0.9263 - val_loss: 0.0651 - val_accuracy: 0.9827\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.2177 - accuracy: 0.9289 - val_loss: 0.0670 - val_accuracy: 0.9812\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.2062 - accuracy: 0.9325 - val_loss: 0.0570 - val_accuracy: 0.9833\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.2064 - accuracy: 0.9339 - val_loss: 0.0628 - val_accuracy: 0.9833\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1938 - accuracy: 0.9378 - val_loss: 0.0506 - val_accuracy: 0.9857\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1925 - accuracy: 0.9380 - val_loss: 0.0652 - val_accuracy: 0.9808\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1888 - accuracy: 0.9381 - val_loss: 0.0589 - val_accuracy: 0.9840\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1806 - accuracy: 0.9417 - val_loss: 0.0631 - val_accuracy: 0.9842\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1906 - accuracy: 0.9385 - val_loss: 0.0613 - val_accuracy: 0.9825\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1785 - accuracy: 0.9410 - val_loss: 0.0546 - val_accuracy: 0.9849\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 633s 352ms/step - loss: 0.1864 - accuracy: 0.9400 - val_loss: 0.0795 - val_accuracy: 0.9790\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1745 - accuracy: 0.9439 - val_loss: 0.0538 - val_accuracy: 0.9849\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1739 - accuracy: 0.9437 - val_loss: 0.0586 - val_accuracy: 0.9832\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1635 - accuracy: 0.9463 - val_loss: 0.0566 - val_accuracy: 0.9872\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1728 - accuracy: 0.9445 - val_loss: 0.0496 - val_accuracy: 0.9870\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.1681 - accuracy: 0.9467 - val_loss: 0.0803 - val_accuracy: 0.9790\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 629s 350ms/step - loss: 0.1606 - accuracy: 0.9493 - val_loss: 0.3973 - val_accuracy: 0.9345\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1617 - accuracy: 0.9486 - val_loss: 0.0586 - val_accuracy: 0.9840\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1669 - accuracy: 0.9482 - val_loss: 0.0500 - val_accuracy: 0.9870\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1466 - accuracy: 0.9531 - val_loss: 0.0707 - val_accuracy: 0.9825\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 629s 350ms/step - loss: 0.1559 - accuracy: 0.9510 - val_loss: 0.0699 - val_accuracy: 0.9857\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1682 - accuracy: 0.9489 - val_loss: 0.0465 - val_accuracy: 0.9868\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 627s 348ms/step - loss: 0.1683 - accuracy: 0.9490 - val_loss: 0.0614 - val_accuracy: 0.9837\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1528 - accuracy: 0.9525 - val_loss: 0.0473 - val_accuracy: 0.9873\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1451 - accuracy: 0.9549 - val_loss: 0.0625 - val_accuracy: 0.9835\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1562 - accuracy: 0.9513 - val_loss: 0.0500 - val_accuracy: 0.9869\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1612 - accuracy: 0.9514 - val_loss: 0.0469 - val_accuracy: 0.9885\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1495 - accuracy: 0.9540 - val_loss: 0.0466 - val_accuracy: 0.9882\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1487 - accuracy: 0.9526 - val_loss: 0.0859 - val_accuracy: 0.9807\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1509 - accuracy: 0.9543 - val_loss: 0.0858 - val_accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 627s 349ms/step - loss: 0.1585 - accuracy: 0.9507 - val_loss: 0.0549 - val_accuracy: 0.9866\n",
      "Epoch 60/100\n",
      "1800/1800 [==============================] - 629s 349ms/step - loss: 0.1458 - accuracy: 0.9542 - val_loss: 0.0933 - val_accuracy: 0.9793\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.1486 - accuracy: 0.9536 - val_loss: 0.0612 - val_accuracy: 0.9843\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1498 - accuracy: 0.9548 - val_loss: 0.0542 - val_accuracy: 0.9885\n",
      "Epoch 63/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.1581 - accuracy: 0.9526 - val_loss: 0.0759 - val_accuracy: 0.9850\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1562 - accuracy: 0.9535 - val_loss: 0.0553 - val_accuracy: 0.9856\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.1475 - accuracy: 0.9540 - val_loss: 0.0813 - val_accuracy: 0.9832\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1499 - accuracy: 0.9538 - val_loss: 0.0614 - val_accuracy: 0.9858\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1470 - accuracy: 0.9558 - val_loss: 0.0531 - val_accuracy: 0.9866\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1451 - accuracy: 0.9549 - val_loss: 0.0645 - val_accuracy: 0.9860\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 640s 355ms/step - loss: 0.1576 - accuracy: 0.9547 - val_loss: 0.0507 - val_accuracy: 0.9881\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 660s 366ms/step - loss: 0.1439 - accuracy: 0.9540 - val_loss: 0.0492 - val_accuracy: 0.9867\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 639s 355ms/step - loss: 0.1526 - accuracy: 0.9540 - val_loss: 0.0912 - val_accuracy: 0.9819\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 703s 390ms/step - loss: 0.1358 - accuracy: 0.9579 - val_loss: 0.0603 - val_accuracy: 0.9831\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 780s 433ms/step - loss: 0.1751 - accuracy: 0.9515 - val_loss: 0.0461 - val_accuracy: 0.9885\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 720s 400ms/step - loss: 0.1303 - accuracy: 0.9590 - val_loss: 0.0472 - val_accuracy: 0.9874\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 768s 427ms/step - loss: 0.1492 - accuracy: 0.9552 - val_loss: 0.0631 - val_accuracy: 0.9847\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 674s 375ms/step - loss: 0.1441 - accuracy: 0.9562 - val_loss: 0.0760 - val_accuracy: 0.9834\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 713s 396ms/step - loss: 0.1572 - accuracy: 0.9537 - val_loss: 0.0429 - val_accuracy: 0.9878\n",
      "Epoch 78/100\n",
      "1800/1800 [==============================] - 653s 362ms/step - loss: 0.1480 - accuracy: 0.9555 - val_loss: 0.0604 - val_accuracy: 0.9862\n",
      "Epoch 79/100\n",
      "1800/1800 [==============================] - 629s 350ms/step - loss: 0.1518 - accuracy: 0.9558 - val_loss: 0.0786 - val_accuracy: 0.9827\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - 628s 349ms/step - loss: 0.1355 - accuracy: 0.9586 - val_loss: 0.0700 - val_accuracy: 0.9844\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 624s 346ms/step - loss: 0.1490 - accuracy: 0.9562 - val_loss: 0.0612 - val_accuracy: 0.9858\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1421 - accuracy: 0.9577 - val_loss: 0.0596 - val_accuracy: 0.9870\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 638s 354ms/step - loss: 0.1484 - accuracy: 0.9557 - val_loss: 0.0545 - val_accuracy: 0.9881\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1400 - accuracy: 0.9573 - val_loss: 0.0474 - val_accuracy: 0.9876\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 635s 352ms/step - loss: 0.1461 - accuracy: 0.9562 - val_loss: 0.0592 - val_accuracy: 0.9864\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 633s 352ms/step - loss: 0.1477 - accuracy: 0.9560 - val_loss: 0.2130 - val_accuracy: 0.9559\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 633s 352ms/step - loss: 0.1409 - accuracy: 0.9567 - val_loss: 0.0782 - val_accuracy: 0.9850\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 633s 352ms/step - loss: 0.1440 - accuracy: 0.9563 - val_loss: 0.0640 - val_accuracy: 0.9846\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1440 - accuracy: 0.9570 - val_loss: 0.2168 - val_accuracy: 0.9659\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 636s 353ms/step - loss: 0.1452 - accuracy: 0.9570 - val_loss: 0.0892 - val_accuracy: 0.9823\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1640 - accuracy: 0.9539 - val_loss: 0.0618 - val_accuracy: 0.9841\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 633s 351ms/step - loss: 0.1442 - accuracy: 0.9576 - val_loss: 0.0718 - val_accuracy: 0.9822\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1472 - accuracy: 0.9573 - val_loss: 0.0757 - val_accuracy: 0.9817\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1376 - accuracy: 0.9594 - val_loss: 0.0527 - val_accuracy: 0.9867\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 632s 351ms/step - loss: 0.1501 - accuracy: 0.9564 - val_loss: 0.0587 - val_accuracy: 0.9849\n",
      "Epoch 96/100\n",
      "1800/1800 [==============================] - 631s 351ms/step - loss: 0.1387 - accuracy: 0.9581 - val_loss: 0.0669 - val_accuracy: 0.9864\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 635s 353ms/step - loss: 0.1406 - accuracy: 0.9581 - val_loss: 0.0658 - val_accuracy: 0.9860\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 641s 356ms/step - loss: 0.1732 - accuracy: 0.9514 - val_loss: 0.0663 - val_accuracy: 0.9861\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 630s 350ms/step - loss: 0.1315 - accuracy: 0.9612 - val_loss: 0.0630 - val_accuracy: 0.9863\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 631s 350ms/step - loss: 0.1434 - accuracy: 0.9572 - val_loss: 0.0617 - val_accuracy: 0.9882\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1800,\n",
    "    batch_size=32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=460,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1054361",
   "metadata": {},
   "source": [
    "### This code trains the CNN model on the training set using the fit() method.\n",
    "\n",
    "* The train_generator and validation_generator variables are the generators that generate batches of training and validation data respectively. These generators use data augmentation techniques such as rotation and horizontal flip to increase the amount of training data and prevent overfitting.\n",
    "\n",
    "* The steps_per_epoch parameter is set to 1800, which is the number of batches of samples in one epoch of training data. The batch_size parameter is set to 32, which is the number of samples in each batch.\n",
    "\n",
    "* The validation_steps parameter is set to 460, which is the number of batches of samples in one epoch of validation data.\n",
    "\n",
    "* The verbose parameter is set to 1, which specifies the verbosity mode.\n",
    "\n",
    "* The epochs parameter is set to 100, which is the number of times the model will be trained on the entire training dataset.\n",
    "\n",
    "* The training progress and evaluation metrics are stored in the history variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27387c64",
   "metadata": {},
   "source": [
    "## Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f854b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('test.jpg')\n",
    "img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33fc858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[7.2037713e-15 0.0000000e+00 6.6051906e-13 0.0000000e+00 1.7434092e-15\n",
      "  9.9999917e-01 0.0000000e+00 0.0000000e+00 4.6236978e-17 0.0000000e+00\n",
      "  1.9962741e-13 7.5853723e-07 1.9431951e-28 6.0373488e-23 2.8219731e-08\n",
      "  1.4931690e-23]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(np.array([img]))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a6913",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23e66f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
