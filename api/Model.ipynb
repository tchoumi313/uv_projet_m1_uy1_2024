{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f014ade8",
   "metadata": {},
   "source": [
    "# **Plant Disease Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105ab64",
   "metadata": {},
   "source": [
    "## Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ff1545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 13:36:45.824847: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 13:36:47.796411: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-19 13:36:48.369763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 13:36:50.096432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3312a5a",
   "metadata": {},
   "source": [
    "\n",
    "* tensorflow is imported as tf.\n",
    "* The models and layers sub-modules are imported from tensorflow.keras. These sub-modules provide functions for creating and training neural network models, as well as building and configuring different types of layers.\n",
    "* matplotlib.pyplot is imported as plt, which allows you to create different types of plots and visualizations to analyze your data.\n",
    "* IPython.display.HTML is imported to display HTML content in the Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc3329",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f071a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c5dff",
   "metadata": {},
   "source": [
    "## Data Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481c5019",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/home/donsoft/.local/lib/python3.11/site-packages/keras/preprocessing/image/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[1;32m      3\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m      4\u001b[0m         rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,\n\u001b[1;32m      5\u001b[0m         rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      6\u001b[0m         horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m         target_size\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE,IMAGE_SIZE),\n\u001b[1;32m     11\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     12\u001b[0m         class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/home/donsoft/.local/lib/python3.11/site-packages/keras/preprocessing/image/__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/train',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aff553",
   "metadata": {},
   "source": [
    "* This code uses the ImageDataGenerator class from tensorflow.keras.preprocessing.image module to perform data augmentation on image data during training of a deep learning model. Data augmentation helps to artificially increase the size of the dataset by creating new, slightly modified versions of the existing images, which helps to improve the robustness of the model and reduce overfitting.\n",
    "\n",
    "* The train_datagen object is created with several parameters to specify the types of image augmentations to apply, such as rotating the image by a random angle between -10 and 10 degrees, and randomly flipping the image horizontally. The rescale parameter is used to normalize the pixel values in the image to be between 0 and 1.\n",
    "\n",
    "* The train_generator object is then created using the flow_from_directory() method, which takes the path to the training directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory, apply the specified augmentations, and generate batches of augmented images for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8172c",
   "metadata": {},
   "source": [
    "## Class Names with Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8011f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f2cd85",
   "metadata": {},
   "source": [
    "## Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b61f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98f22f",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'dataset/val',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a7c9d",
   "metadata": {},
   "source": [
    "* This code creates a validation generator similar to the train_generator, but for the validation set. The validation_datagen object is created with the same set of parameters as train_datagen for consistency.\n",
    "\n",
    "* The validation_generator is created using the flow_from_directory() method, which takes the path to the validation directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for validation during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eae32c",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df606de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'dataset/test',\n",
    "        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        batch_size=32,\n",
    "        class_mode=\"sparse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa68d0",
   "metadata": {},
   "source": [
    "* This code creates a test generator similar to the train_generator and validation_generator. The test_datagen object is created with the same set of parameters as train_datagen and validation_datagen for consistency.\n",
    "\n",
    "* The test_generator is created using the flow_from_directory() method, which takes the path to the test directory, the target size of the images (IMAGE_SIZE x IMAGE_SIZE), the batch size, and the class mode (sparse in this case, which means that the labels are integers). The generator will read images from the directory and generate batches of images for testing the model after training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae35dd7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 16\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aa21f0d",
   "metadata": {},
   "source": [
    "* The model is created using the Sequential API from Keras, which allows us to stack layers sequentially to define the architecture of the CNN model.\n",
    "* The InputLayer specifies the shape of the input tensor to the model, which is determined by the input_shape variable.\n",
    "* The Conv2D layers are convolutional layers that apply filters (also known as kernels) to the input images to extract local patterns or features. The first Conv2D layer has 32 filters, the second has 64 filters, and the third has 128 filters. All three Conv2D layers use a 3x3 kernel and the relu activation function, which introduces non-linearity to the model.\n",
    "* The MaxPooling2D layers perform max pooling operation, which downsamples the feature maps by selecting the maximum value from a 2x2 window. This helps to reduce the spatial dimensions and capture the most important features.\n",
    "* The Flatten layer is used to convert the feature maps into a 1D array. This flattening operation is necessary to feed the data into a fully connected layer.\n",
    "* The Dense layer is a fully connected layer that has 32 units and uses the relu activation function. It takes the flattened feature maps as input and applies weights and biases to produce output values.\n",
    "* The Dropout layer is used to mitigate overfitting, which is a common issue in deep neural networks. It randomly sets a certain percentage (in this case, 50%) of the input units to 0 at each training iteration to prevent the model from relying too much on any single input unit.\n",
    "* The final Dense layer has n_classes units, where n_classes is the number of classes in the dataset. It uses the softmax activation function, which produces a probability distribution over the classes as the output of the model. The class with the highest probability is predicted as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b7d44",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9b80e",
   "metadata": {},
   "source": [
    "## Model Compile with Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160e9a0",
   "metadata": {},
   "source": [
    "### This code compiles the CNN model defined in the previous code block using the compile() method.\n",
    "\n",
    "* The optimizer parameter is set to 'adam', which is an optimization algorithm that is commonly used for deep learning models.\n",
    "\n",
    "* The loss parameter is set to tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), which is the loss function used to measure the difference between the predicted and actual labels. SparseCategoricalCrossentropy is used because the labels are in integer form, and not one-hot encoded.\n",
    "\n",
    "* The metrics parameter is set to 'accuracy', which specifies that the accuracy of the model will be used as the evaluation metric during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dceef",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ae58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=1800,\n",
    "    batch_size=32,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=460,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1054361",
   "metadata": {},
   "source": [
    "### This code trains the CNN model on the training set using the fit() method.\n",
    "\n",
    "* The train_generator and validation_generator variables are the generators that generate batches of training and validation data respectively. These generators use data augmentation techniques such as rotation and horizontal flip to increase the amount of training data and prevent overfitting.\n",
    "\n",
    "* The steps_per_epoch parameter is set to 1800, which is the number of batches of samples in one epoch of training data. The batch_size parameter is set to 32, which is the number of samples in each batch.\n",
    "\n",
    "* The validation_steps parameter is set to 460, which is the number of batches of samples in one epoch of validation data.\n",
    "\n",
    "* The verbose parameter is set to 1, which specifies the verbosity mode.\n",
    "\n",
    "* The epochs parameter is set to 100, which is the number of times the model will be trained on the entire training dataset.\n",
    "\n",
    "* The training progress and evaluation metrics are stored in the history variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27387c64",
   "metadata": {},
   "source": [
    "## Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f854b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('test.jpg')\n",
    "img = cv2.resize(img, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "img = img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(np.array([img]))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a6913",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e66f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
